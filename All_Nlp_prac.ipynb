{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 1 a) Installing NLTK.\n",
    "# b) Write python script to convert given input text to speech.\n",
    "# c) Write python script to convert given input speech to text.\n",
    "#b) Convert the given text to speech.\n",
    "Source code:\n",
    "# text to speech\n",
    "# pip install gtts\n",
    "# pip install playsound\n",
    "from playsound import playsound\n",
    "# import required for text to speech conversion\n",
    "from gtts import gTTS\n",
    "mytext = \"Welcome to Natural Language programming\"\n",
    "language = \"en\"\n",
    "myobj = gTTS(text=mytext, lang=language, slow=False)\n",
    "myobj.save(\"myfile.mp3\")\n",
    "playsound(\"myfile.mp3\")\n",
    "\n",
    "#c) Convert audio file Speech to Text.\n",
    "Source code:\n",
    "Note: required to store the input file \"male.wav\" in the current folder before running the program.\n",
    "#pip3 install SpeechRecognition pydub\n",
    "import speech_recognition as sr\n",
    "filename = \"male.wav\"\n",
    "# initialize the recognizer\n",
    "r = sr.Recognizer()\n",
    "# open the file\n",
    "with sr.AudioFile(filename) as source:\n",
    "# listen for the data (load audio to memory)\n",
    "audio_data = r.record(source)\n",
    "# recognize (convert from speech to text)\n",
    "text = r.recognize_google(audio_data)\n",
    "print(text)\n",
    "\n",
    "# b) Write python script to convert given input text to speech.\n",
    "\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "text_val='All the best for your exam.'\n",
    "language='en'\n",
    "obj=gTTS (text=text_val, lang-language, slow=False)\n",
    "obj.save (\"exam.mp3\")\n",
    "playsound (\"exam.mp3\")\n",
    "\n",
    "#Speech to text\n",
    "import speech_recognition as sr\n",
    "filename=\"C:/Users/tcsc/Desktop/NLP PRACT/female.flac\"\n",
    "r=sr.Recognizer ()\n",
    "with sr.AudioFile (filename) as source:\n",
    "audio_data=r.record (source)\n",
    "text=r.recognize_google (audio_data)\n",
    "print (text)\n",
    "#this is an example of synthesized speech that was created by a neural Ho\n",
    "\n",
    "import speech_recognition as sr\n",
    "filename=\"C:/Users/tcsc/Downloads/Alice_Arnold_voice.ogg\"\n",
    "r=sr.Recognizer ()\n",
    "with sr.AudioFile (filename) as source:\n",
    "audio_data=r.record (source)\n",
    "text=r.recognize_google (audio_data)\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRACTICAL NO. 2\n",
    "# a) Study of various corpus-Brown, Inaugural, Reuters, UDHR with various methods like fields, raw, words, sents, categories.\n",
    "# b) Study Conditional Frequency Distribution.\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "brown.words ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.categories ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words (categories= 'mystery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words (categories='religion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words (categories='fiction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words (categories= 'humor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.fileids ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words (fileids=['cg22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words (fileids=['cg23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.sents ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.sents (categories=['mystery', 'fiction', 'humor', 'religion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as brown corpus do for reuters inaugural udhr\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "reuters.words ()\n",
    "reuters.categories ()\n",
    "mystery=reuters.words(categories= 'barley')\n",
    "print(mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Study Conditional Frequency Distribution.\n",
    "news_text=brown.words (categories='news')\n",
    "fdist=nltk. FreqDist ( [w.lower () for w in news_text])\n",
    "modals=['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print (m+':', fdist [m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd=nltk.ConditionalFreqDist((genre, word)\n",
    "for genre in brown.categories ()\n",
    "for word in brown.words (categories=genre))\n",
    "genres=['news', 'religion', 'fiction', 'thriller', 'romance']\n",
    "modals=['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate (conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 3\n",
    "# a) Create and use your own corpora (plain text).\n",
    "import nltk.corpus\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "# Define the directory containing your plain text files\n",
    "corpus_dir = '/path/to/your/corpus/directory'\n",
    "\n",
    "# Create a PlaintextCorpusReader\n",
    "custom_corpus = PlaintextCorpusReader(corpus_dir, '.*\\.txt')\n",
    "\n",
    "# List all files in the corpus\n",
    "print(\"Files in custom corpus:\", custom_corpus.fileids())\n",
    "\n",
    "# Access text from specific files\n",
    "for fileid in custom_corpus.fileids():\n",
    "    print(\"Contents of\", fileid, \":\", custom_corpus.raw(fileid)[:100])\n",
    "\n",
    "# b) Study of tagged corpora with methods like tagged_sents, tagged_words.\n",
    "# c) WAP to find the most frequent noun tags.\n",
    "\n",
    "# a) Create and use your own corpora (plain text).\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root='C:/Users/tcsc/Downloads/Lisa/NLTK Practicals/MYCorpus' filelist=PlaintextCorpus Reader (corpus_root,'*')\n",
    "print('\\n File list: \\n')\n",
    "\n",
    "print (filelist.fileids ())\n",
    "\n",
    "print('\\n Filelist Root-'+filelist.root)\n",
    "\n",
    "w=filelist.words('MSCIT.txt')\n",
    "print (w[:6])\n",
    "\n",
    "wl=filelist.sents('MSCIT.txt')\n",
    "print(w1)\n",
    "\n",
    "print ('\\n\\nStatistics for each text: \\n')\n",
    "\n",
    "print ('AvgWordLen\\tAvgSentence Len\\tno. ofTimeEachWordAppearsOnAvg\\tFileName')\n",
    "for fileid in filelist.fileids ():\n",
    "    num_chars=len (filelist.raw (fileid))\n",
    "    num_words=len (filelist.words (fileid))\n",
    "    num_sents=len (filelist.sents (fileid))\n",
    "    num_vocab=len (set ([w.lower() for w in filelist.words (fileid)]))\n",
    "    print(int (num_chars/num_words), '\\t\\t\\t', int (num_words/num_sents), '\\t\\t\\t', int (num_words/num_vocab), '\\t\\t', fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Study of tagged corpora with methods like tagged_sents, tagged_words.\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.corpus.brown.tagged_words ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.conll2000.tagged_words ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.treebank.tagged_words ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.treebank. tagged_sents ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) WAP to find the most frequent noun tags.\n",
    "from nltk.corpus import treebank\n",
    "wsj=nltk.corpus.treebank.tagged_words()\n",
    "word_tag_fd=nltk.FreqDist(wsj)\n",
    "noun_pairs = [(word.lower(), tag) for word, tag in wsj if tag.startswith('NN')]\n",
    "\n",
    "# Compute the frequency distribution of noun-word pairs\n",
    "noun_tags = [tag for word, tag in wsj if tag.startswith('NN')]\n",
    "\n",
    "# Compute the frequency distribution of noun tags\n",
    "noun_tag_fd = nltk.FreqDist(noun_tags)\n",
    "\n",
    "# Print the most common noun tags\n",
    "print(\"Most Frequent Noun Tags:\")\n",
    "for tag, freq in noun_tag_fd.most_common():\n",
    "    print(tag, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 4\n",
    "# a) Map Words to Properties using Python Dictionaries.\n",
    "# b) Study (i)DefaultTagger (ii) Regular Expression Tagger (iii) UnigramTagger.\n",
    "\n",
    "#(i)DefaultTagger\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "exptagger = DefaultTagger('NN')\n",
    "from nltk.corpus import treebank\n",
    "testsentences = treebank.tagged_sents() [1000:]\n",
    "print(exptagger.evaluate (testsentences))\n",
    "#Tagging a list of sentences\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "exptagger = DefaultTagger('NN')\n",
    "print(exptagger.tag_sents([['Hi', ','], ['How', 'are', 'you', '?']]))\n",
    "\n",
    "#ii. Regular Expression Tagger\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import RegexpTagger\n",
    "test_sent = brown.sents(categories='news')[0]\n",
    "regexp_tagger = RegexpTagger(\n",
    "[(r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers\n",
    "(r'(The|the|A|a|An|an)$', 'AT'), # articles\n",
    "(r'.*able$', 'JJ'), # adjectives\n",
    "(r'.*ness$', 'NN'), # nouns formed from adjectives\n",
    "(r'.*ly$', 'RB'), # adverbs\n",
    "(r'.*s$', 'NNS'), # plural nouns\n",
    "(r'.*ing$', 'VBG'), # gerunds\n",
    "(r'.*ed$', 'VBD'), # past tense verbs\n",
    "(r'.*', 'NN') # nouns (default)\n",
    "])\n",
    "print(regexp_tagger)\n",
    "print(regexp_tagger.tag(test_sent))\n",
    "#iii. unigram\n",
    "# Loading Libraries\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "# Training using first 10 tagged sentences of the treebank corpus as data.\n",
    "# Using data\n",
    "train_sents = treebank.tagged_sents()[:10]\n",
    "# Initializing\n",
    "tagger = UnigramTagger(train_sents)\n",
    "# Lets see the first sentence\n",
    "# (of the treebank corpus) as list\n",
    "print(treebank.sents()[0])\n",
    "print('\\n',tagger.tag(treebank.sents()[0]))\n",
    "#Finding the tagged results after training.\n",
    "tagger.tag(treebank.sents()[0])\n",
    "#Overriding the context model\n",
    "tagger = UnigramTagger(model ={'Pierre': 'NN'})\n",
    "print('\\n',tagger.tag(treebank.sents()[0]))\n",
    "\n",
    "# a) Map Words to Properties using Python Dictionaries.\n",
    "import nltk\n",
    "pos={}\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos['colorless']='ADJ'\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos['ideas']='N'\n",
    "pos['sleep']='V'\n",
    "pos['furiously']='ADJ'\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pos)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sorted(pos):\n",
    "    print(word+\".\",pos[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos['sleep']=['N','V']\n",
    "pos.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=dict(colorless='ADJ',ideas='N',sleep='V',furiously='ADJ')\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Study (i)DefaultTagger (ii) Regular Expression Tagger (iii) UnigramTagger.\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "tags=[tag for(word,tag)in brown.tagged_words(categories='news')]\n",
    "nltk.FreqDist(tags).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw='i do not like green eggs and ham, i do not like them Sam i am!'\n",
    "tokens=nltk.word_tokenize(raw)\n",
    "default_tagger=nltk.DefaultTagger('NN')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = default_tagger.tag(tokens)\n",
    "print(\"Tagged tokens with default tagger:\", tagged_tokens)\n",
    "\n",
    "# Evaluate the default tagger on the Brown corpus\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "accuracy = default_tagger.accuracy(brown_tagged_sents)\n",
    "print(\"Accuracy of the default tagger on 'news' category of Brown corpus:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 5\n",
    "#AIM: a) Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms. \n",
    "#b) Study of lemmas, hyponyms, hypernyms, meronyms, entailments. \n",
    "#c) WAP to find synonym and antonym of word 'active' using Wordnet.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from colab ai\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Get synsets for the word \"dog\"\n",
    "synsets = wordnet.synsets(\"dog\")\n",
    "\n",
    "# Print the synsets\n",
    "for synset in synsets:\n",
    "    print(\"Synset:\", synset)\n",
    "\n",
    "# Get the definition of the first synset\n",
    "definition = synsets[0].definition()\n",
    "\n",
    "# Print the definition\n",
    "print(\"Definition:\", definition)\n",
    "\n",
    "# Get examples of the first synset\n",
    "examples = synsets[0].examples()\n",
    "\n",
    "# Print the examples\n",
    "print(\"Examples:\")\n",
    "for example in examples:\n",
    "    print(\"- {}\".format(example))\n",
    "\n",
    "# Get antonyms of the first synset\n",
    "antonyms = synsets[0].lemmas()[0].antonyms()\n",
    "\n",
    "# Print the antonyms\n",
    "print(\"Antonyms:\")\n",
    "for antonym in antonyms:\n",
    "    print(\"- {}\".format(antonym.name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('motocar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('car.n.01').lemma_names()\n",
    "\n",
    "wn.synset('car.n.01').examples()\n",
    "\n",
    "wn.synset('car.n.01').definition()\n",
    "\n",
    "wn.synset('car.n.01').lemmas\n",
    "\n",
    "wn.synset('car.n.01').lemmas()\n",
    "\n",
    "wn.lemma('car.n.01.automobile')\n",
    "\n",
    "wn.lemma('car.n.01.automobile').synset()\n",
    "\n",
    "wn.lemma('car.n.01.automobile').name()\n",
    "wn.lemmas('car')\n",
    "\n",
    "for synset in wn.synsets('car'):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Study of lemmas, hyponyms, hypernyms, meronyms, entailments.\n",
    "car=wn.synset('car.n.01')\n",
    "car\n",
    "\n",
    "types_of_car=car.hyponyms()\n",
    "types_of_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_of_car[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([lemma.name() for synset in types_of_car for lemma in synset.lemmas()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths=car.hypernym_paths()\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('tree.n.02').part_meronyms()\n",
    "wn.synset('tree.n.01').substance_meronyms()\n",
    "wn.synset('tree.n.01').member_holonyms()\n",
    "wn.synset('walk.v.01').entailments()\n",
    "wn.synset('eat.v.01').entailments()\n",
    "wn.lemma('supply.n.02.supply').antonyms()\n",
    "\n",
    "wn.lemma('supply.n.02.supply').antonyms()\n",
    "\n",
    "wn.lemma('supply.n.02.supply').antonyms()\n",
    "\n",
    "wn.lemma('rush.n.02.rush').antonyms()\n",
    "\n",
    "wn.lemma('horizontal.a.01.horizontal').antonyms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) WAP to find synonym and antonym of word 'active' using Wordnet.\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "print(wordnet.synsets(\"active\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordnet.lemma('active.a.01.active').antonyms())\n",
    "synsets = wordnet.synsets(\"active\")\n",
    "\n",
    "# Print the synsets\n",
    "print(\"Synsets for 'active':\")\n",
    "for synset in synsets:\n",
    "    print(synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Find synsets (sets of synonyms) for the word 'active'\n",
    "synsets_active = wordnet.synsets(\"active\")\n",
    "\n",
    "# Print synonyms for 'active'\n",
    "print(\"Synonyms for 'active':\")\n",
    "for synset in synsets_active:\n",
    "    for lemma in synset.lemmas():\n",
    "        print(lemma.name())\n",
    "\n",
    "# Print antonyms for 'active'\n",
    "print(\"\\nAntonyms for 'active':\")\n",
    "for synset in synsets_active:\n",
    "    for lemma in synset.lemmas():\n",
    "        if lemma.antonyms():\n",
    "            print(lemma.antonyms()[0].name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 6\n",
    "# a) Compare two nouns.\n",
    "# b) Handling stopword.\n",
    "\n",
    "# a) Compare two nouns.\n",
    "\n",
    "import nltk\n",
    "nltk.download ('wordnet')\n",
    "nltk.download ('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn1=wordnet.synsets(\"football\")\n",
    "syn2=wordnet.synsets ('soccer')\n",
    "for s1 in syn1:\n",
    "    for s2 in syn2:\n",
    "        print(\"Path similarity of: \")\n",
    "        print (s1, '(',s1.pos (), ')','[', sl.definition (), ']')\n",
    "        print (s2, '(',s2.pos (), ')', '[',s2.definition (), ']')\n",
    "        print(\" is\", s1.path_similarity (s2))\n",
    "        print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b\n",
    "#(i) Adding or Removing Stop Words in NLTK’s Default Stop Word List\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download ('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "print (stopwords.words ())\n",
    "nltk.download ('punkt')\n",
    "text=\" messi likes to play football, however he is not too fond of tennis\"\n",
    "text_tokens=word_tokenize(text)\n",
    "token_without_sw= [word for word in text_tokens if not word in stopwords.words ()]\n",
    "print (token_without_sw)\n",
    "all_stopwords=stopwords.words ('english')\n",
    "all_stopwords.append('play')\n",
    "text_tokens=word_tokenize(text)\n",
    "token_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
    "print (token_without_sw)\n",
    "all_stopwords.remove('is')\n",
    "text_tokens=word_tokenize(text)\n",
    "token_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
    "print (token_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(ii) Using Gensim Adding and Removing Stop Words in Default Gensim Stop Words List.\n",
    "#gensim\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "text=\" messi likes to play football, however he is not too fond of tennis\"\n",
    "filtered_sentence=remove_stopwords (text)\n",
    "print (filtered_sentence)\n",
    "all_stopwords=gensim.parsing.preprocessing.STOPWORDS\n",
    "print (all_stopwords)\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "all_stopwords_gensim=STOPWORDS.union (set(['likes', 'play']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens=word_tokenize (text)\n",
    "token_without_sw=[word for word in text_tokens if not word in all_stopwords_gensim]\n",
    "print (token_without_sw)\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "all_stopwords_gensim=STOPWORDS\n",
    "sw_list={\"not\"}\n",
    "all_stopwords_gensim-STOPWORDS.difference (sw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens=word_tokenize (text)\n",
    "token_without_sw=[word for word in text_tokens if not word in all_stopwords_gensim]\n",
    "print (token_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(iii) Using Spacy Adding and Removing Stop Words in Default Spacy Stop Words List\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sp=spacy.load('en_core_web_sm')\n",
    "all_stopwords=sp.Defaults.stop_words\n",
    "all_stopwords.add(\"play\")\n",
    "text=\" messi likes to play football, however he is not too fond of tennis\"\n",
    "text_tokens=word_tokenize (text)\n",
    "token_without_sw=[word for word in text_tokens if not word in all_stopwords]\n",
    "print (token_without_sw)\n",
    "all_stopwords.remove('to')\n",
    "token_without_sw=[word for word in text_tokens if not word in all_stopwords]\n",
    "print (token_without_sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iii\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the default stop words set\n",
    "default_stop_words = set(nlp.Defaults.stop_words)\n",
    "\n",
    "# Print the default stop words before modification\n",
    "print(\"Default stop words before modification:\", default_stop_words)\n",
    "\n",
    "# Add a new stop word\n",
    "new_stop_word = \"customstopword\"\n",
    "default_stop_words.add(new_stop_word)\n",
    "\n",
    "# Print the default stop words after adding the new stop word\n",
    "print(\"Default stop words after adding:\", default_stop_words)\n",
    "\n",
    "# Check if the stop word \"my\" is in the default stop words list\n",
    "print(\"Is 'my' in default stop words list:\", 'my' in default_stop_words)\n",
    "\n",
    "# Remove a stop word\n",
    "removed_stop_word = \"my\"\n",
    "default_stop_words.discard(removed_stop_word)\n",
    "\n",
    "# Print the default stop words after removing\n",
    "print(\"Default stop words after removing:\", default_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 7 \n",
    "#Text Tokenization\n",
    "# a) Tokenization using Python’s split() function\n",
    "text=\"this tool is on a beta stage, alexa developers can use get metrics\"\n",
    "data=text.split()\n",
    "for i in data:\n",
    "    print(i)\n",
    "\n",
    "# b) Tokenization using Regular Expressions (Regfx)\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tk=RegexpTokenizer('s+',gaps=True)\n",
    "str=\"i love to study NLP in Python\"\n",
    "tokens=tk.tokenize(str)\n",
    "print(tokens)  \n",
    "\n",
    "# c) Tokenization using NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "str=\"i love to study DL\"\n",
    "print(word_tokenize(str))\n",
    "\n",
    "# d) Tokenization using the spaCy library\n",
    "import spacy\n",
    "nlp=spacy.blank(\"en\")\n",
    "str=\"i love nlp\"\n",
    "doc=nlp(str)\n",
    "words=[word.text for word in doc]\n",
    "print(words)\n",
    "\n",
    "# e) Tokenization using Keras\n",
    "import keras\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "str=\"i love to study NLP\"\n",
    "tokens=text_to_word_sequence(str)\n",
    "print(tokens)\n",
    "\n",
    "# f) Tokenization using Gensim\n",
    "#pip install gensim\n",
    "from gensim.utils import tokenize\n",
    "str=\"i love to study nlp\"\n",
    "list(tokenize(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 8 Import NLP Libraries for Indian Languages and perform:\n",
    "# a) Word tokenization in Hindi.\n",
    "# b) Generate similar sentences from a given Hindi text input.\n",
    "# c) Identify the Indian language of a text.\n",
    "\n",
    "# a) Word tokenization in Hindi.\n",
    "\n",
    "# !pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#pip install torch==2.1.2\n",
    "# !pip install inltk\n",
    "# !pip install tornado==4.5.3\n",
    "from inltk.inltk import setup\n",
    "setup('hi')\n",
    "from inltk.inltk import tokenize\n",
    "hindi_text = \"\"\"प्राकृ तिक भाषा सीखना बहुत तिलचस्प है।\"\"\"\n",
    "# tokenize(input text, language code)\n",
    "tokenize(hindi_text, \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Generate similar sentences from a given Hindi text input.\n",
    "\n",
    "from inltk.inltk import setup\n",
    "setup('hi')\n",
    "from inltk.inltk import get_similar_sentences\n",
    "# get similar sentences to the one given in hindi\n",
    "output = get_similar_sentences('मैंआज बहुत खुश हूं', 5, 'hi')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Identify the Indian language of a text.\n",
    "from inltk.inltk import setup\n",
    "setup('gu')\n",
    "from inltk.inltk import identify_language\n",
    "#Identify the Lnaguage of given text\n",
    "identify_language('બીના કાપડિયા')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inltk.inltk import identify_language\n",
    "\n",
    "# Define the text\n",
    "text = \"भारत में विभिन्न भाषाएँ बोली जाती हैं।\"\n",
    "\n",
    "# Identify the language of the text\n",
    "language = identify_language(text)\n",
    "\n",
    "# Print the identified language\n",
    "print(\"Identified language:\", language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 9\n",
    "# Illustrate POS tagging:\n",
    "# a) Sentence tokenization, word tokenization, part of speech tagging and chunking of user define text.\n",
    "# b) Name Entity Recognition of user defined text.\n",
    "# c) Name Entity Recognition with diagram using NLTK corpus-treebank.\n",
    "\n",
    "#a\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk import tag\n",
    "from nltk import chunk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para=\"Today we will be learning NLTK.\"\n",
    "sents=tokenize.sent_tokenize (para)\n",
    "print(\"\\nsentence tokenization\\n=========\\n\",sents)\n",
    "words=tokenize.word_tokenize (para)\n",
    "\n",
    "print(\"\\nword tokenization\\n=========\\n\",words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(sents)):\n",
    "    words=tokenize.word_tokenize (sents [index])\n",
    "    print(words)\n",
    "#POS Tagging\n",
    "print(\"\\nPOS tagging\\n==========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words=[]\n",
    "for index in range(len(sents)):\n",
    "    tagged_words.append(tag.pos_tag(words))\n",
    "    print(tagged_words)\n",
    "#chunking\n",
    "print(\"\\nChunking\\n==========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=[]\n",
    "for index in range(len(sents)):\n",
    "    tree.append(chunk.ne_chunk(tagged_words [index]))\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Name Entity Recognition of user defined text.\n",
    "\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "text=\"\"\"Apple Inc., originally named Apple doing Computer, Inc.,\n",
    "is a multinational corporation that creates and markets consumer electronics and attendant computer software, and is a digital distributor of media content. Apple's core product lines are the iPhone smartphone, iPad tablet computer, and the Macintosh personal computer.\n",
    "The company offers its products online and has a chain of retail stores known as Apple Stores. Founders Steve Jobs, Steve Wozniak, and Ronald Wayne created Apple Computer Co. on April 1, 1976, to market Wozniak's Apple I desktop computer, [2] and Jobs and Wozniak incorporated the company on January 3, 1977, [3] in Cupertino, California.\"\"\"\n",
    "doc=nlp(text)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Print the named entities\n",
    "print(\"Named Entities:\")\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the user-defined text\n",
    "text = \"Apple Inc. is headquartered in Cupertino, California.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Print the named entities\n",
    "print(\"Named Entities:\")\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Name Entity Recognition with diagram using NLTK corpus-treebank.\n",
    "\n",
    "#pip install svgling\n",
    "import nltk\n",
    "nltk.download(\"treebank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "treebank_chunk.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_chunk.chunked_sents()[0]\n",
    "#treebank_chunk.chunked_sents()[0].draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
